Title: Gauge Theory for Convolutional Neural Networks

Speaker: Mats Hansen

Abstract: This talk builds a bridge from classical gauge theory to geometric deep learning (GDL). We begin with a quick primer on fibre, vector, and principal bundles; Ehresmann vs. Levi–Civita connections; and the connection 1-form and curvature, highlighting the geometric idea of vertical/horizontal splittings. We then reinterpret modern neural networks through this lens: a data domain (e.g., an image grid) is the base space, feature channels form fibres, and a “gauge” is a local choice of basis in each fibre. Gauge transformations—changes of local frames—should not alter semantics, so network blocks are designed to be gauge-equivariant. Concretely, a 1×1 convolution is a per-point linear map f:R3→RC that must intertwine input/output representations (f∘ρin=ρout∘f) so outputs transform predictably under local channel permutations (e.g., S3 for RGB). For spatial operators, features live in different fibres; principled aggregation therefore requires parallel transport before combining neighbours, yielding gauge-equivariant message passing/convolutions. The slides provide the minimal gauge-theoretic toolkit and illustrate these ideas on an RGB example and the GDL blueprint.